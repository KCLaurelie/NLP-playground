# -*- coding: utf-8 -*-
"""Copy of TODO - Neural Networks Intro.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18tagweI3A2txRvLiODa3dFVeocs9PwO0
"""

# SWICH RUNTIME TO GPU


import numpy as np
import sklearn.metrics

# For ploting 
import matplotlib.pyplot as plt
import seaborn as sns

# Get torch stuff
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim

np.random.seed(42)


# **Activation functions**


def step(z):
  return np.array(z > 0, dtype=np.int32)

def tanh(z):
  return np.tanh(z)

def sigmoid(z):
  return 1 / (1+np.exp(-z))
  

def relu(z):
  return np.maximum(0,z)

def plot(z, yh, name):
  plt.grid(True)
  plt.xlabel('z')
  plt.ylabel('Å·')
  plt.plot(z, yh, label=name, color='b')
  plt.axhline(0, color='orange')
  plt.axvline(0, color='orange')
  plt.legend()
  plt.show()

z = np.linspace(-10, 10, 100)

yh = step(z)
plot(z, yh, 'Step')

yh = relu(z)
plot(z, yh, 'ReLU')

yh = sigmoid(z)
plot(z, yh, 'Sigmoid')

"""# **An example of an oversimplified problem that can be solved using NN**

Our input is outside temperature which is a value between -30 and 40. We want our NN to predict is it hot or cold outside. This is an example of supervised ML using NN.

Let's encode the output values as:

$
hot   -  1 \\
cold - 0
$

---

$
D = \{\\
(-4, 0), \\
(1, 0),\\
(25, 1),\\
...\}
$

---
"""

# TODO: Generate a vector with 1000 integers with values from -30 to 40
x = np.random.randint(-30,40,1000)
y = np.array([1 if v > 20 else 0 for v in x])
for i in range(10):
  print("If the temperature is {:4} it is  {}".format(x[i], "hot" if y[i] == 1 else "cold"))

"""# Initialize the parameters
The first thing we need to do is initialize the parameters of our NN. In our case we only have two parameters $w_1$ and $b_1$. Usually a random valaue between -1 and 1 is assigned to both.
"""

#Set seed, so that we always have the same result
np.random.seed(13)

# TODO: Initialize the parameters of the Neural Network
w1 = np.random.uniform(-1,1) # we want a random number between -1 and 1
b1 = np.random.uniform(-1,1)
print(w1,b1)

"""$
Calculate the outputs, set sigmoid as the activation function.
"""

#TODO 
# Forward pass - get the predictions
z1 = w1*x+b1
yh = sigmoid(z1)
print(z1[0:3])
print()
print(yh[0:3])

"""# Because we are using the sigmod function we can interpret the output as probability"""

# Decode the outputs hot/cold
yh_temp = ['hot' if val > 0.5 else 'cold' for val in yh]

# Print the first 10 g.t. and predictions
for i in range(10):
  print("If the temperature is {:4} it is  {:4}  vs predicted  {}".format(
      x[i], "hot" if y[i] == 1 else "cold", yh_temp[i]))

"""# Cost function / Loss function / Error function
"""

# Calculate the loss
loss = sklearn.metrics.log_loss(y, yh)
print(loss)

"""# **1. Prepare the dataset**"""

device = torch.device('cpu') # for GPU: use 'cuda'

# The whole dataset
print(x[0:10])
print(y[0:10])
print()
# Print the size
print("The size of our dataset is: " + str(x.size))

"""---


**Generate a train/test/dev dataset**
"""

inds = np.random.permutation(len(x))
inds_train = inds[0:int(0.8*len(x))]
inds_test = inds[int(0.8*len(x)):int(0.9*len(x))]
inds_dev = inds[int(0.9*len(x)):]

# 80% of the dataset
x_train = x[inds_train]
y_train = y[inds_train]

# 10% of the dataset
x_test = x[inds_test]
y_test = y[inds_test]

# 10% of the dataset
x_dev = x[inds_dev]
y_dev = y[inds_dev]

"""---


**Convert the inputs to PyTorch**
"""

x_train = torch.tensor(x_train.reshape(-1, 1), dtype=torch.float32)
y_train = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)

x_dev = torch.tensor(x_dev.reshape(-1, 1), dtype=torch.float32)
y_dev = torch.tensor(y_dev.reshape(-1, 1), dtype=torch.float32)

x_test = torch.tensor(x_test.reshape(-1, 1), dtype=torch.float32)
y_test = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)

"""# Build the Neural Network"""

class Net(nn.Module):
    def __init__(self):
      super(Net, self).__init__()
      self.fc1 = nn.Linear(1, 1) #1 layer with 1 neuron (nb inputs, nb outputs)
      
    def forward(self, x): # forward passing
      x = torch.sigmoid(self.fc1(x)) # linear layer receives as input x
      return x

"""# Initialize the NN

We also create the criterion (loss function) and the optimizer
"""

net = Net() # instantiate the class
criterion = nn.BCELoss() # loss function (binary cross entropy loss or log loss)
optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.999) # thing that updates the weights (backpropagation calcultaes, optimizer updates the weights)
# here optimizer = sigmoid gradient descent with learning rate or 0.0001
# also popular: adam optimizer

# fc1 = first layer (fc=fully connected: all layers in pytorch are fully connected)
print(net.fc1.weight) 
print(net.fc1.bias)

# requires_grad=True: means this parameter needs to be learnt

"""# Train"""

net.to(device) # move the network to device (CPU)
x_train = x_train.to(device)
y_train = y_train.to(device)

x_dev = x_dev.to(device)
y_dev = y_dev.to(device)


net.train() # push network in training mode
# things we want to track
losses = [] #loss
accs = [] #accuracy
ws = [] #weight
bs = [] # biases


for epoch in range(10000):  # do 200 epoch 
  # re-initializes the gradient to 0
  optimizer.zero_grad() #pytorch remembers gradients from previous steps, otherwise gradients will be updated with previous updates

  # Forward 
  outputs = net(x_train)
  # Calculate error
  loss = criterion(outputs, y_train) #outputs = y_hat intermediate
  # Backward
  loss.backward()
  # Optimize/Update parameters
  optimizer.step()
  
  # Track the changes - This is normally done using tensorboard or similar
  losses.append(loss.item())
  accs.append(sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy()))
  ws.append(net.fc1.weight.cpu().detach().numpy()[0][0])
  bs.append(net.fc1.bias.cpu().detach().numpy()[0])

  # print statistics
  if epoch % 500 == 0:
      acc = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy())
      
      outputs_dev = net(x_dev)
      acc_dev = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs_dev.cpu().detach().numpy()], y_dev.cpu().numpy())
      
      print("Epoch: {:4} Loss: {:.5f} Acc: {:.3f} Acc Dev: {:.3f}".format(epoch, loss.item(), acc, acc_dev))

print('Finished Training')


# overfitting: accuracy very high, acc dev very low

"""# Plot Everything"""

fig = plt.figure()
fig.subplots_adjust(hspace=0.6, wspace=0.6)
fig.set_size_inches(10, 10)
plt.subplot(2, 2, 1)
sns.lineplot(np.arange(0, len(bs)), bs).set_title("Bias")

plt.subplot(2, 2, 2)
sns.lineplot(np.arange(0, len(ws)), ws).set_title("Weight")

# TODO: Add two plots for the Loss and Accuracy, place them in the right positions
plt.subplot(2, 2, 3)
sns.lineplot(np.arange(0, len(losses)), losses).set_title("loss")

plt.subplot(2, 2, 4)
sns.lineplot(np.arange(0, len(accs)), accs).set_title("accuracy")


fig.show()

# what we should see: until the end they stabilize/plateau
# loss needs to reach plateau (otherwise need to continue training)

"""# Let's do multiple layers"""

#TODO - Create a network with 3 Layers for each use sigmoid as activation:
#L1 - 4 Neurons
#L2 - 3 Neurons
#L3 - 1 Neuron

#Add sigmod as the activation function
import torch.optim as optim
class Net(nn.Module):
    def __init__(self):
      super(Net, self).__init__()
      self.fc1 = nn.Linear(1, 4) #1 layer with 1 input (1 variable: temperature), 4 outputs (4 neurons)
      self.fc2 = nn.Linear(4, 3) #1 layer with 4 inputs, 3 outputs
      self.fc3 = nn.Linear(3, 1) # 3 inputs, 1 output (only 1 prediction)
      
    def forward(self, x):
      x1 = torch.sigmoid(self.fc1(x)) # linear layer receives as input x
      x2 = torch.sigmoid(self.fc2(x1))
      x3 = torch.sigmoid(self.fc3(x2))
      
      return x3

net = Net()
criterion = nn.BCELoss()
optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.999)

print(net.fc2.weight)

net.to(device)
x_train = x_train.to(device)
y_train = y_train.to(device)
net.train()
losses = []
accs = []
ws = []
bs = []
for epoch in range(10000):  # do 200 epoch 
  # zero the gradients
  optimizer.zero_grad()

  # Forward 
  outputs = net(x_train)
  # Calculate error
  loss = criterion(outputs, y_train)
  # Backward
  loss.backward()
  # Optimize/Update parameters
  optimizer.step()
  
  # Track the changes - This is normally done using tensorboard or similar
  losses.append(loss.item())
  accs.append(sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy()))
  ws.append(net.fc1.weight.cpu().detach().numpy()[0][0])
  bs.append(net.fc1.bias.cpu().detach().numpy()[0])

  # print statistics
  if epoch % 500 == 0:
      acc = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy())
      print("Epoch: {:4} Loss: {:.5} Acc: {:.3}".format(epoch, loss.item(), acc))

print('Finished Training')

fig = plt.figure()
fig.subplots_adjust(hspace=0.6, wspace=0.6)
fig.set_size_inches(10, 10)
plt.subplot(2, 2, 1)
sns.lineplot(np.arange(0, len(bs)), bs).set_title("Bias")

plt.subplot(2, 2, 2)
sns.lineplot(np.arange(0, len(ws)), ws).set_title("Weight")

plt.subplot(2, 2, 3)
sns.lineplot(np.arange(0, len(losses)), losses).set_title("loss")

plt.subplot(2, 2, 4)
sns.lineplot(np.arange(0, len(accs)), accs).set_title("accuracy")


fig.show()

net.eval() # switch network to evaluation mode
net.to("cpu") # switch back to cpu
net(torch.tensor([22],dtype=torch.float32)) # 22: input temperature
net(torch.tensor([100],dtype=torch.float32)) # how well the network generalizes with never seen temperatures