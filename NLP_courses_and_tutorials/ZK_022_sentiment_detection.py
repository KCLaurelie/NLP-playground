# -*- coding: utf-8 -*-
"""Sentiment detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
        https://colab.research.google.com/drive/12mRPcTl1yf6uEhza-yq6nZe2mAQ-9Bcf
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from gensim.models import Word2Vec
# Get torch stuff
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
import torch
import sklearn.metrics
import spacy
from spacy.attrs import LOWER

# Basic configuration for logging - needed for gensim to print some output
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# Load the english model for spacy
nlp = spacy.load('en', disable=['ner', 'parser'])

"""# Get the data from github"""

df = pd.read_csv("https://raw.githubusercontent.com/w-is-h/tmp/master/dataset.csv", encoding='cp1252')

df.head()

"""# Print some statistics"""

print("The shape of the dataset is: " + str(df.shape))
print("The columns are: " + str(list(df.columns)))
print("Number of positive values: " + str(np.sum(df['Sentiment'])))
_ = plt.hist(df['Sentiment'])

"""**We'll take only 10,000 examples, to speed thigns up a bit**"""

x = df['SentimentText'].values[0:10000]
y = df['Sentiment'].values[0:10000]

"""# Spacy"""

doc = nlp("I was running yesterday.")

print(doc)

# To access tokens we can loop over the document
for token in doc:
    print(token)

# Each token has multiple properties, e.g. lower_
for token in doc:
    print(token.lower_)

"""# Split sentences into tokens and lowercase"""

# Print the first sentence
print(x[0])

# Lowercase and tokenize
x = [[tkn.lower_ for tkn in nlp(snt)] for snt in x]

# Print the first sentence
print(x[0])

# Train word2vec

w2v = Word2Vec(x, size=100, window=6, min_count=5, workers=4)

w2v.wv.most_similar("good")
w2v.wv.get_vector("the")

"""# Convert each sentence into the average sum of its tokens"""

# x_emb - embedded sentences
x_emb = np.zeros((len(x), 100))
# Loop over sentences
for i_snt, snt in enumerate(x):
    cnt = 0
    # Loop over the words of a sentence
    for i_word, word in enumerate(snt):
        if word in w2v.wv:
            x_emb[i_snt] += w2v.wv.get_vector(word)
            cnt += 1
    if cnt > 0:
        x_emb[i_snt] = x_emb[i_snt] / cnt

"""# Build and initialize the network"""

device = torch.device('cuda')
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(100, 300)
        self.fc2 = nn.Linear(300, 30)
        self.fc3 = nn.Linear(30, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.sigmoid(self.fc3(x))
        return x


net = Net()
criterion = nn.BCELoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.99)

"""# Split the dataset into train/test/dev"""

inds = np.random.permutation(len(x))
inds_train = inds[0:int(0.8*len(x))]
inds_test = inds[int(0.8*len(x)):int(0.9*len(x))]
inds_dev = inds[int(0.9*len(x)):]

# 80% of the dataset
x_train = x_emb[inds_train]
y_train = y[inds_train]

# 10% of the dataset
x_test = x_emb[inds_test]
y_test = y[inds_test]

# 10% of the dataset
x_dev = x_emb[inds_dev]
y_dev = y[inds_dev]

x_train = torch.tensor(x_train, dtype=torch.float32)
y_train = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)

x_dev = torch.tensor(x_dev, dtype=torch.float32)
y_dev = torch.tensor(y_dev.reshape(-1, 1), dtype=torch.float32)

x_test = torch.tensor(x_test, dtype=torch.float32)
y_test = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)

"""# Train"""

net.to(device)
x_train = x_train.to(device)
y_train = y_train.to(device)

x_dev = x_dev.to(device)
y_dev = y_dev.to(device)


net.to(device)
x_train = x_train.to(device)
y_train = y_train.to(device)

net.train()
losses = []
accs = []
ws = []
bs = []
for epoch in range(10000):    # do 10,000 epochs 
    # zero the gradients
    optimizer.zero_grad()

    # Forward 
    outputs = net(x_train)
    # Calculate error
    loss = criterion(outputs, y_train)
    # Backward
    loss.backward()
    # Optimize/Update parameters
    optimizer.step()

    # Track the changes - This is normally done using tensorboard or similar
    losses.append(loss.item())
    accs.append(sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy()))
    ws.append(net.fc1.weight.cpu().detach().numpy()[0][0])
    bs.append(net.fc1.bias.cpu().detach().numpy()[0])

    # print statistics
    if epoch % 500 == 0:
        acc = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy())

        outputs_dev = net(x_dev)
        acc_dev = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs_dev.cpu().detach().numpy()], y_dev.cpu().numpy())

        print("Epoch: {:4} Loss: {:.5f} Acc: {:.3f} Acc Dev: {:.3f}".format(epoch, loss.item(), acc, acc_dev))

print('Finished Training')


fig = plt.figure()
fig.subplots_adjust(hspace=0.6, wspace=0.6)
fig.set_size_inches(10, 10)
plt.subplot(2, 2, 1)
sns.lineplot(np.arange(0, len(bs)), bs).set_title("Bias")
plt.subplot(2, 2, 2)
sns.lineplot(np.arange(0, len(ws)), ws).set_title("Weight")
plt.subplot(2, 2, 3)
sns.lineplot(np.arange(0, len(losses)), losses).set_title("Loss")
plt.subplot(2, 2, 4)
sns.lineplot(np.arange(0, len(accs)), accs).set_title("Acc")
fig.show()

net.eval()
net.to("cpu")

net(torch.tensor(x_emb[3], dtype=torch.float32))

print(y[3])
print(df['SentimentText'].iloc[3])

